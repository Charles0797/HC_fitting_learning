# é¦–å…ˆæœç´¢æ¡†æœç´¢configæŸ¥çœ‹é…ç½®æ˜¯å¦éœ€è¦æ›´æ”¹
# å†æœç´¢pthæŸ¥çœ‹è®­ç»ƒæ¨¡å‹æ˜¯å¦é…ç½®ä¸€è‡´
# æœç´¢æ¡†æœç´¢steel01dataæŸ¥çœ‹æ–‡ä»¶æ˜¯å¦ä¸‹è½½ï¼Œä½ç½®æ˜¯å¦æ­£ç¡®
# æœç´¢æ¡†æœç´¢ï¼ˆ250,15000,0.1ï¼‰3åœˆsteel01ä¸‹è½½ä½ç½®æ˜¯å¦æ­£ç¡®
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Steel01 å‚æ•°è¯†åˆ« â€” ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé€‚é…8æ ¸16GBé…ç½®ï¼‰
ä¸»è¦ä¼˜åŒ–ï¼š
- Windowså¤šè¿›ç¨‹å…¼å®¹æ€§ä¿®å¤
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼ˆ16GBå‹å¥½ï¼‰
- OpenSeesç¨³å®šæ€§å¢å¼º
- ç³»ç»Ÿèµ„æºç›‘æ§
- é”™è¯¯å¤„ç†æ”¹è¿›
"""

import os
import sys
import tempfile
import shutil
import gzip
import pickle
import logging
import gc
import glob
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from collections import deque
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import pandas as pd
from scipy.signal import find_peaks
from vacation_task.improve.test_config import get_test_config

# å°è¯•å¯¼å…¥psutilç”¨äºç³»ç»Ÿç›‘æ§ï¼ˆå¦‚æœæ²¡æœ‰å¯ä»¥pip install psutilï¼‰
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("è­¦å‘Š: æœªå®‰è£…psutilï¼Œæ— æ³•è¿›è¡Œç³»ç»Ÿèµ„æºç›‘æ§ã€‚å»ºè®®è¿è¡Œ: pip install psutil")

# ------------------- æ—¥å¿—é…ç½® -------------------
LOGFILE = "training_log.txt"
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s [%(levelname)s] %(message)s',
                    handlers=[logging.StreamHandler(), logging.FileHandler(LOGFILE, encoding='utf-8')])
logger = logging.getLogger(__name__)


# ------------------- ç³»ç»Ÿèµ„æºç›‘æ§ -------------------
def monitor_system_resources():
    """
    ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
    è¿”å›ç³»ç»ŸCPUã€å†…å­˜ä½¿ç”¨æƒ…å†µçš„å­—å…¸
    """
    if not PSUTIL_AVAILABLE:
        return None

    try:
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)

        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_available = memory.available / (1024 ** 3)  # GB

        # å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µ
        process = psutil.Process()
        process_memory = process.memory_info().rss / (1024 ** 3)  # GB

        logger.info(f"ç³»ç»Ÿç›‘æ§ - CPU: {cpu_percent:.1f}%, "
                    f"å†…å­˜: {memory_percent:.1f}% (å¯ç”¨: {memory_available:.1f}GB), "
                    f"è¿›ç¨‹å†…å­˜: {process_memory:.2f}GB")

        # å†…å­˜ä½¿ç”¨è­¦å‘Š
        if memory_percent > 85:
            logger.warning("âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼å»ºè®®å‡å°‘batch_sizeæˆ–buffer_size")

        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'memory_available_gb': memory_available,
            'process_memory_gb': process_memory
        }
    except Exception as e:
        logger.warning(f"æ— æ³•è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯: {e}")
        return None


def force_garbage_collection():
    """
    å¼ºåˆ¶è¿›è¡Œåƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
    """
    collected = gc.collect()
    logger.info(f"åƒåœ¾å›æ”¶å®Œæˆï¼Œé‡Šæ”¾äº† {collected} ä¸ªå¯¹è±¡")


# ------------------- æ•°æ®åŠ è½½ -------------------
def load_training_data_from_txt(txt_file_path):
    """
    ä»txtæ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®

    å‚æ•°:
        txt_file_path: txtæ–‡ä»¶è·¯å¾„

    è¿”å›:
        åŒ…å«è®­ç»ƒæ•°æ®çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«paramsã€protocolã€curveã€filename
    """
    logger.info(f"æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {txt_file_path} (æ–°æ ¼å¼)")

    if not os.path.exists(txt_file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {txt_file_path}")
        return []

    try:
        with open(txt_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

    if len(lines) < 12010:
        logger.error(f"é”™è¯¯: æ–‡ä»¶è¡Œæ•°ä¸è¶³ ({len(lines)}è¡Œ)ï¼Œéœ€è¦è‡³å°‘12010è¡Œ")
        return []

    num_columns = len(lines[0].split('\t'))
    all_data = []
    logger.info(f"æ£€æµ‹åˆ° {num_columns} ä¸ªæ ·æœ¬åˆ—")

    # é€åˆ—è§£ææ•°æ®
    for col_idx in range(num_columns):
        try:
            # è¯»å–å‚æ•°ï¼ˆå‰3è¡Œï¼‰
            fy = float(lines[0].split('\t')[col_idx])
            E = float(lines[1].split('\t')[col_idx])
            b = float(lines[2].split('\t')[col_idx])

            # è¯»å–ä½ç§»æ•°æ®ï¼ˆç¬¬10-6009è¡Œï¼Œå…±6000ä¸ªç‚¹ï¼‰
            displacement = []
            for row_idx in range(10, 6010):
                if row_idx < len(lines):
                    parts = lines[row_idx].split('\t')
                    if col_idx < len(parts) and parts[col_idx].strip():
                        displacement.append(float(parts[col_idx]))

            # è¯»å–åŠ›æ•°æ®ï¼ˆç¬¬6010-12009è¡Œï¼Œå…±6000ä¸ªç‚¹ï¼‰
            force = []
            for row_idx in range(6010, 12010):
                if row_idx < len(lines):
                    parts = lines[row_idx].split('\t')
                    if col_idx < len(parts) and parts[col_idx].strip():
                        force.append(float(parts[col_idx]))

            # æ•°æ®é•¿åº¦éªŒè¯
            if len(displacement) != 6000 or len(force) != 6000:
                logger.warning(f"æ ·æœ¬ {col_idx + 1} æ•°æ®é•¿åº¦ä¸åŒ¹é… - ä½ç§»: {len(displacement)} ç‚¹, åŠ›: {len(force)} ç‚¹")
                if len(displacement) > 6000:
                    displacement = displacement[:6000]
                if len(force) > 6000:
                    force = force[:6000]
                if len(displacement) < 6000 or len(force) < 6000:
                    continue

            # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨
            all_data.append({
                'params': [fy, E, b],
                'protocol': np.array(displacement, dtype=np.float32),
                'curve': np.array(force, dtype=np.float32),
                'filename': f'data_{col_idx + 1}'
            })

            # è¿›åº¦æç¤º
            if (col_idx + 1) % 10 == 0:
                logger.info(f"å·²åŠ è½½æ ·æœ¬ {col_idx + 1}/{num_columns}")

        except Exception as e:
            logger.exception(f"åŠ è½½æ ·æœ¬ {col_idx + 1} æ—¶å‡ºé”™: {e}")
            continue

    logger.info(f"æ€»å…±åŠ è½½äº† {len(all_data)} ç»„è®­ç»ƒæ•°æ®")

    # æ‰“å°å‚æ•°èŒƒå›´ç»Ÿè®¡
    if all_data:
        params_array = np.array([d['params'] for d in all_data])
        logger.info("å‚æ•°èŒƒå›´ç»Ÿè®¡ï¼š")
        names = ['fy', 'E', 'b']
        for i, n in enumerate(names):
            logger.info(
                f"{n}: min={params_array[:, i].min():.2f}, max={params_array[:, i].max():.2f}, mean={params_array[:, i].mean():.2f}")

    return all_data


def load_test_data_from_excel(excel_file_path):
    """
    ä»Excelæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®

    å‚æ•°:
        excel_file_path: Excelæ–‡ä»¶è·¯å¾„

    è¿”å›:
        tuple: (displacement, force) æ•°ç»„
    """
    if not os.path.exists(excel_file_path):
        logger.error(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file_path}")
        return np.array([]), np.array([])

    try:
        df = pd.read_excel(excel_file_path)

        # å°è¯•ä¸åŒçš„åˆ—åç»„åˆ
        if 'displacement' in df.columns and 'force' in df.columns:
            displacement = df['displacement'].values
            force = df['force'].values
        elif 'ä½ç§»' in df.columns and 'åŠ›' in df.columns:
            displacement = df['ä½ç§»'].values
            force = df['åŠ›'].values
        else:
            # ä½¿ç”¨å‰ä¸¤åˆ—
            displacement = df.iloc[:, 0].values
            force = df.iloc[:, 1].values

        logger.info(f"æˆåŠŸåŠ è½½æµ‹è¯•æ•°æ®: {len(displacement)} ä¸ªæ•°æ®ç‚¹")
        return displacement.astype(np.float32), force.astype(np.float32)

    except Exception as e:
        logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
        return np.array([]), np.array([])


# ------------------- æ”¹è¿›çš„å¤šè¿›ç¨‹Worker -------------------
def worker_hysteretic_batch_improved(args):
    """
    æ”¹è¿›çš„å¤šè¿›ç¨‹workerï¼Œç”¨äºå¹¶è¡Œè®¡ç®—æ»å›æ›²çº¿
    å¢å¼ºäº†é”™è¯¯å¤„ç†å’Œç¨³å®šæ€§

    å‚æ•°:
        args: tuple (params_list, protocol)

    è¿”å›:
        np.array: shape (len(params_list), n_steps) çš„åŠ›å“åº”æ•°ç»„
    """
    params_list, protocol = args

    # è¿›ç¨‹çº§åˆ«çš„å¼‚å¸¸å¤„ç†
    try:
        # é‡æ–°å¯¼å…¥openseesï¼ˆé¿å…è¿›ç¨‹é—´å†²çªï¼‰
        import openseespy.opensees as ops

        # ç¡®ä¿æ¯ä¸ªworkerè¿›ç¨‹ä¸­openseesæ˜¯å¹²å‡€çš„
        try:
            ops.wipe()
        except:
            pass

    except ImportError as e:
        print(f"Workerè¿›ç¨‹æ— æ³•å¯¼å…¥opensees: {e}")
        return np.zeros((len(params_list), len(protocol)), dtype=float)
    except Exception as e:
        print(f"Workeråˆå§‹åŒ–å¤±è´¥: {e}")
        return np.zeros((len(params_list), len(protocol)), dtype=float)

    protocol = np.asarray(protocol, dtype=np.float64)
    n_steps = len(protocol)
    n_models = len(params_list)
    results = np.zeros((n_models, n_steps), dtype=float)

    try:
        # OpenSeesæ¨¡å‹è®¾ç½®
        ops.model('basic', '-ndm', 1, '-ndf', 1)
        ops.node(1, 0.0)
        ops.node(2, 0.0)
        ops.fix(1, 1)

        # åˆ›å»ºææ–™å’Œå•å…ƒï¼ˆæ”¯æŒå¤šä¸ªå‚æ•°ç»„åˆï¼‰
        valid_elements = []
        for i, p in enumerate(params_list, start=1):
            try:
                fy, E, b = p
                # å‚æ•°æœ‰æ•ˆæ€§éªŒè¯
                if fy <= 0 or E <= 0 or b <= 0 or fy > 10000 or E > 100000 or b > 1:
                    print(f"å‚æ•°æ— æ•ˆ: fy={fy}, E={E}, b={b}")
                    continue

                mat_id = i
                ele_id = i
                ops.uniaxialMaterial('Steel01', mat_id, float(fy), float(E), float(b))
                ops.element('twoNodeLink', ele_id, 1, 2, '-mat', mat_id, '-dir', 1)
                valid_elements.append(i)

            except Exception as mat_e:
                print(f"åˆ›å»ºææ–™/å•å…ƒ {i} å¤±è´¥: {mat_e}")
                continue

        if not valid_elements:
            print("æ²¡æœ‰æœ‰æ•ˆçš„å•å…ƒè¢«åˆ›å»º")
            ops.wipe()
            return results

        # åˆ†æè®¾ç½®
        ops.timeSeries('Linear', 1)
        ops.pattern('Plain', 1, 1)
        ops.load(2, 1.0)
        ops.algorithm('Newton')
        ops.numberer('RCM')
        ops.constraints('Transformation')
        ops.system('BandSPD')
        ops.integrator("DisplacementControl", 2, 1, 0.0)
        ops.analysis('Static')

        # é€æ­¥åˆ†æ
        prev_disp = 0.0
        consecutive_failures = 0
        max_consecutive_failures = 10  # å…è®¸çš„è¿ç»­å¤±è´¥æ¬¡æ•°

        for step_idx, disp in enumerate(protocol):
            delta_disp = float(disp - prev_disp)
            prev_disp = disp

            try:
                ops.integrator('DisplacementControl', 2, 1, delta_disp)
                ok = ops.analyze(1)

                if ok == 0:
                    consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                    # æå–æ¯ä¸ªå•å…ƒçš„åŠ›
                    for i in valid_elements:
                        try:
                            fvec = ops.eleForce(i)
                            if isinstance(fvec, (list, tuple, np.ndarray)):
                                val = float(fvec[0]) if len(fvec) > 0 else 0.0
                            else:
                                val = float(fvec)
                            # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                            if np.isnan(val) or np.isinf(val):
                                val = 0.0
                        except Exception:
                            val = 0.0
                        results[i - 1, step_idx] = val
                else:
                    consecutive_failures += 1
                    # è¿ç»­å¤±è´¥å¤ªå¤šæ¬¡åˆ™æå‰ç»ˆæ­¢
                    if consecutive_failures > max_consecutive_failures:
                        print(f"è¿ç»­å¤±è´¥æ¬¡æ•°è¶…è¿‡ {max_consecutive_failures}ï¼Œæå‰ç»ˆæ­¢åˆ†æ")
                        break
                    # è®¾ç½®å½“å‰æ­¥çš„åŠ›ä¸º0
                    for i in valid_elements:
                        results[i - 1, step_idx] = 0.0

            except Exception as step_e:
                print(f"åˆ†ææ­¥éª¤ {step_idx} å¤±è´¥: {step_e}")
                consecutive_failures += 1
                if consecutive_failures > max_consecutive_failures:
                    break

        ops.wipe()

    except Exception as e:
        print(f"Worker OpenSeesåˆ†æå¤±è´¥: {e}")
        try:
            ops.wipe()
        except:
            pass

    return results


# ------------------- ä¼˜åŒ–çš„å¤šç›®æ ‡æ»å›ç¯å¢ƒ -------------------
class MultiTargetHysteresisEnvOptimized:
    """
    ä¼˜åŒ–ç‰ˆçš„å¤šç›®æ ‡æ»å›ç¯å¢ƒç±»
    - å‡å°‘äº†ç‰¹å¾ç»´åº¦ä»¥é™ä½å†…å­˜ä½¿ç”¨
    - æ·»åŠ äº†ç»“æœç¼“å­˜æœºåˆ¶
    - æ”¹è¿›äº†é”™è¯¯å¤„ç†
    """

    def __init__(self, target_data, feature_dim=30):  # ä»50å‡å°‘åˆ°30
        # Steel01ææ–™å‚æ•°èŒƒå›´
        self.param_ranges = [
            [1, 1000],  # fy: å±ˆæœå¼ºåº¦
            [1, 50000],  # E: å¼¹æ€§æ¨¡é‡
            [0.01, 0.3]  # b: åº”å˜ç¡¬åŒ–æ¯”
        ]
        # å‚æ•°è°ƒæ•´çš„æ¯”ä¾‹å› å­
        self.scale_factors = [10, 500, 0.005]

        self.target_data = target_data
        self.num_targets = len(target_data) if target_data else 0
        self.feature_dim = feature_dim

        self.current_target_idx = None
        self.current_target = None
        self.current_params = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_count = 0
        self.success_count = 0
        self.opensees_failure_count = 0
        self.opensees_total_count = 0

        # ç»“æœç¼“å­˜ï¼ˆå‡å°‘é‡å¤è®¡ç®—ï¼‰
        self._curve_cache = {}
        self.max_cache_size = 500  # é™åˆ¶ç¼“å­˜å¤§å°é¿å…å†…å­˜æº¢å‡º

    def reset(self, target_idx=None):
        """é‡ç½®ç¯å¢ƒåˆ°æ–°çš„episode"""
        self.episode_count += 1

        if self.num_targets == 0:
            self.current_params = self.random_params()
            return self._get_state()

        # é€‰æ‹©ç›®æ ‡
        if target_idx is None:
            self.current_target_idx = np.random.randint(self.num_targets)
        else:
            self.current_target_idx = target_idx

        self.current_target = self.target_data[self.current_target_idx]
        self.current_params = self.random_params()

        return self._get_state()

    def random_params(self):
        """ç”Ÿæˆéšæœºçš„ææ–™å‚æ•°"""
        return [np.random.uniform(low, high) for low, high in self.param_ranges]

    def normalize_params(self, params):
        """å°†å‚æ•°å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´"""
        norm = []
        for p, (low, high) in zip(params, self.param_ranges):
            norm_p = 2 * (p - low) / (high - low) - 1
            norm.append(np.clip(norm_p, -1, 1))  # ç¡®ä¿åœ¨èŒƒå›´å†…
        return np.array(norm, dtype=np.float32)

    def _get_state(self):
        """è·å–å½“å‰çŠ¶æ€ï¼ˆç‰¹å¾å‘é‡ï¼‰"""
        norm_params = self.normalize_params(self.current_params)

        if self.current_target is None:
            return np.concatenate([norm_params, np.zeros(self.feature_dim + 10, dtype=np.float32)]).astype(np.float32)

        # æå–ç›®æ ‡æ›²çº¿ç‰¹å¾
        target_features = self._extract_curve_features_optimized(
            self.current_target['curve'],
            self.current_target['protocol']
        )

        # è®¡ç®—å½“å‰å‚æ•°å¯¹åº”çš„æ›²çº¿
        current_curve = self.hysteretic_curve_single(self.current_params)

        # è®¡ç®—è¯¯å·®ç‰¹å¾
        error_features = self._compute_error_features(current_curve, self.current_target['curve'])

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        state = np.concatenate([norm_params, target_features, error_features])
        return state.astype(np.float32)

    def _extract_curve_features_optimized(self, curve, protocol):
        """
        ä¼˜åŒ–çš„æ›²çº¿ç‰¹å¾æå–ï¼ˆå‡å°‘è®¡ç®—é‡å’Œç‰¹å¾ç»´åº¦ï¼‰
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hash((tuple(curve[:100]), tuple(protocol[:100])))  # åªç”¨å‰100ä¸ªç‚¹è®¡ç®—hash
        if cache_key in self._curve_cache:
            return self._curve_cache[cache_key]

        features = []

        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾ (5ä¸ª)
        features.extend([
            np.max(curve), np.min(curve),
            np.mean(curve), np.std(curve),
            np.max(curve) - np.min(curve)
        ])

        # é›¶äº¤å‰ç‚¹æ•°é‡ (1ä¸ª)
        zero_crossings = np.where(np.diff(np.sign(curve)))[0]
        features.append(len(zero_crossings))

        # å³°å€¼æ•°é‡ (1ä¸ª)
        try:
            peaks, _ = find_peaks(np.abs(curve))
            features.append(len(peaks))
        except Exception:
            features.append(0)

        # èƒ½é‡ç‰¹å¾ (1ä¸ª)
        try:
            energy = np.trapz(np.abs(curve), np.abs(protocol))
            features.append(energy)
        except Exception:
            features.append(0)

        # åˆå§‹åˆšåº¦ (1ä¸ª)
        n_initial = min(max(len(curve) // 20, 5), len(curve))
        if n_initial >= 2:
            try:
                initial_slope, _ = np.polyfit(protocol[:n_initial], curve[:n_initial], 1)
                features.append(initial_slope)
            except:
                features.append(0)
        else:
            features.append(0)

        # ç®€åŒ–çš„é‡‡æ ·ç‰¹å¾ (12ä¸ªï¼Œä»20å‡å°‘åˆ°12)
        n_samples = 12
        if len(curve) >= n_samples:
            indices = np.linspace(0, len(curve) - 1, n_samples, dtype=int)
            sampled = curve[indices]
            curve_range = np.max(np.abs(curve)) + 1e-6
            features.extend((sampled / curve_range).tolist())
        else:
            features.extend(curve.tolist())
            features.extend([0] * (n_samples - len(curve)))

        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è°ƒæ•´é•¿åº¦
        features = np.array(features, dtype=np.float32)
        if len(features) < self.feature_dim:
            features = np.pad(features, (0, self.feature_dim - len(features)))
        else:
            features = features[:self.feature_dim]

        # ç¼“å­˜ç»“æœï¼ˆæ§åˆ¶ç¼“å­˜å¤§å°ï¼‰
        if len(self._curve_cache) < self.max_cache_size:
            self._curve_cache[cache_key] = features
        elif len(self._curve_cache) >= self.max_cache_size:
            # æ¸…ç†ä¸€åŠç¼“å­˜
            keys_to_remove = list(self._curve_cache.keys())[:self.max_cache_size // 2]
            for k in keys_to_remove:
                del self._curve_cache[k]
            self._curve_cache[cache_key] = features

        return features

    def _compute_error_features(self, current_curve, target_curve):
        """è®¡ç®—å½“å‰æ›²çº¿ä¸ç›®æ ‡æ›²çº¿çš„è¯¯å·®ç‰¹å¾"""
        min_len = min(len(current_curve), len(target_curve))
        if min_len == 0:
            return np.zeros(10, dtype=np.float32)

        current_curve = current_curve[:min_len]
        target_curve = target_curve[:min_len]
        diff = current_curve - target_curve
        abs_diff = np.abs(diff)

        features = []

        # åŸºæœ¬è¯¯å·®ç»Ÿè®¡ (4ä¸ª)
        features.extend([
            np.mean(abs_diff),
            np.max(abs_diff),
            np.sqrt(np.mean(diff ** 2)),  # RMSE
            np.std(diff)
        ])

        # ç¬¦å·ç»Ÿè®¡ (2ä¸ª)
        features.extend([
            np.sum(diff > 0) / (len(diff) + 1),
            np.sum(diff < 0) / (len(diff) + 1)
        ])

        # åˆ†æ®µè¯¯å·® (4ä¸ª)
        n_segments = 4
        segment_size = max(1, len(diff) // n_segments)
        for i in range(n_segments):
            start_idx = i * segment_size
            end_idx = (i + 1) * segment_size if i < n_segments - 1 else len(diff)
            seg_err = np.mean(abs_diff[start_idx:end_idx]) if end_idx > start_idx else 0.0
            features.append(seg_err)

        return np.array(features, dtype=np.float32)

    def hysteretic_curve_single(self, params, protocol=None):
        """å•ä¸ªå‚æ•°ç»„åˆçš„æ»å›æ›²çº¿è®¡ç®—ï¼ˆåŒ…è£…batchç‰ˆæœ¬ï¼‰"""
        res = self.hysteretic_curve_batch([params], protocol=protocol)
        return res[0] if len(res) > 0 else np.array([])

    def hysteretic_curve_batch(self, params_list, protocol=None):
        """
        æ‰¹é‡è®¡ç®—æ»å›æ›²çº¿ï¼ˆå•è¿›ç¨‹ç‰ˆæœ¬ï¼‰
        ä½¿ç”¨OpenSeesçš„twoNodeLinkå•å…ƒæ‰¹é‡è®¡ç®—å¤šä¸ªå‚æ•°ç»„åˆ

        å‚æ•°:
            params_list: å‚æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º[fy, E, b]
            protocol: ä½ç§»å†ç¨‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰ç›®æ ‡çš„protocol

        è¿”å›:
            np.array: shape (n_models, n_steps) çš„åŠ›å“åº”æ•°ç»„
        """
        try:
            import openseespy.opensees as ops
        except Exception as e:
            logger.exception("æ— æ³•å¯¼å…¥ openseespy: %s", e)
            # å¦‚æœOpenSeesä¸å¯ç”¨ï¼Œè¿”å›é›¶æ•°ç»„
            protocol = np.asarray(protocol) if protocol is not None else (
                self.current_target['protocol'] if self.current_target else np.zeros(100)
            )
            n_steps = len(protocol)
            n_models = len(params_list)
            return np.zeros((n_models, n_steps), dtype=float)

        if protocol is None:
            protocol = self.current_target['protocol'] if self.current_target is not None else np.zeros(100)

        protocol = np.asarray(protocol, dtype=np.float64)
        n_steps = len(protocol)
        n_models = len(params_list)
        results = np.zeros((n_models, n_steps), dtype=float)

        try:
            # æ¸…ç†å¹¶å»ºç«‹æ–°æ¨¡å‹
            ops.wipe()
            ops.model('basic', '-ndm', 1, '-ndf', 1)
            ops.node(1, 0.0)
            ops.node(2, 0.0)
            ops.fix(1, 1)

            # ä¸ºæ¯ä¸ªå‚æ•°ç»„åˆåˆ›å»ºææ–™å’Œå•å…ƒ
            valid_elements = []
            for i, p in enumerate(params_list, start=1):
                try:
                    fy, E, b = p

                    # å‚æ•°æœ‰æ•ˆæ€§æ£€æŸ¥
                    if not (0 < fy < 10000 and 0 < E < 100000 and 0 < b < 1):
                        logger.warning(f"å‚æ•°è¶…å‡ºåˆç†èŒƒå›´: fy={fy}, E={E}, b={b}")
                        continue

                    mat_id = i
                    ele_id = i
                    ops.uniaxialMaterial('Steel01', mat_id, float(fy), float(E), float(b))
                    ops.element('twoNodeLink', ele_id, 1, 2, '-mat', mat_id, '-dir', 1)
                    valid_elements.append(i)

                except Exception as e:
                    logger.warning(f"åˆ›å»ºææ–™/å•å…ƒ {i} å¤±è´¥: {e}")
                    continue

            if not valid_elements:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å•å…ƒè¢«åˆ›å»º")
                ops.wipe()
                return results

            # è®¾ç½®åˆ†æ
            ops.timeSeries('Linear', 1)
            ops.pattern('Plain', 1, 1)
            ops.load(2, 1.0)
            ops.algorithm('Newton')
            ops.numberer('RCM')
            ops.constraints('Transformation')
            ops.system('BandSPD')
            ops.integrator("DisplacementControl", 2, 1, 0.0)
            ops.analysis('Static')

            # é€æ­¥åŠ è½½
            prev_disp = 0.0
            for step_idx, disp in enumerate(protocol):
                delta_disp = float(disp - prev_disp)
                prev_disp = disp

                try:
                    ops.integrator('DisplacementControl', 2, 1, delta_disp)
                    ok = ops.analyze(1)
                    self.opensees_total_count += 1

                    if ok == 0:
                        # æˆåŠŸåˆ†æï¼Œæå–æ¯ä¸ªå•å…ƒçš„åŠ›
                        for i in valid_elements:
                            try:
                                fvec = ops.eleForce(i)
                                if isinstance(fvec, (list, tuple, np.ndarray)):
                                    if len(fvec) == 0:
                                        val = 0.0
                                    else:
                                        val = float(fvec[0])
                                else:
                                    val = float(fvec)

                                # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                                if np.isnan(val) or np.isinf(val):
                                    val = 0.0

                            except Exception:
                                val = 0.0

                            results[i - 1, step_idx] = val
                    else:
                        # åˆ†æå¤±è´¥ï¼Œè®¾ç½®åŠ›ä¸º0
                        self.opensees_failure_count += 1
                        for i in valid_elements:
                            results[i - 1, step_idx] = 0.0

                except Exception as e:
                    logger.warning(f"æ­¥éª¤ {step_idx} åˆ†æå¤±è´¥: {e}")
                    self.opensees_failure_count += 1
                    for i in valid_elements:
                        results[i - 1, step_idx] = 0.0

            ops.wipe()

        except Exception as e:
            logger.exception("[hysteretic_curve_batch] OpenSees åˆ†æå‡ºé”™: %s", e)
            try:
                ops.wipe()
            except Exception:
                pass
            return np.zeros((n_models, n_steps), dtype=float)

        return results

    # å…¼å®¹æ€§æ¥å£
    def hysteretic_curve(self, params, protocol=None):
        """å…¼å®¹æ€§æ¥å£ï¼Œè°ƒç”¨å•ä¸ªæ›²çº¿è®¡ç®—"""
        return self.hysteretic_curve_single(params, protocol=protocol)

    def step(self, action):
        """
        æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œæ­¥éª¤

        å‚æ•°:
            action: åŠ¨ä½œå‘é‡ [delta_fy, delta_E, delta_b]

        è¿”å›:
            tuple: (next_state, reward, done, info)
        """
        # æ ¹æ®åŠ¨ä½œæ›´æ–°å‚æ•°
        new_params = []
        hit_boundary = False

        for i, (p, a) in enumerate(zip(self.current_params, action)):
            delta = self.scale_factors[i] * a
            new_p = p + delta
            low, high = self.param_ranges[i]

            if new_p <= low:
                new_p = low
                hit_boundary = True
            elif new_p >= high:
                new_p = high
                hit_boundary = True

            new_params.append(new_p)

        self.current_params = new_params

        # å¦‚æœæ²¡æœ‰ç›®æ ‡ï¼Œè¿”å›é»˜è®¤å€¼
        if self.current_target is None:
            next_state = self._get_state()
            return next_state, 0, False, {}

        # è®¡ç®—å½“å‰å‚æ•°å¯¹åº”çš„æ›²çº¿
        fitted_curve = self.hysteretic_curve(self.current_params)
        target_curve = self.current_target['curve']

        # è®¡ç®—è¯¯å·®
        min_len = min(len(fitted_curve), len(target_curve))
        if min_len == 0:
            error_norm = float('inf')
            relative_error = 1.0
        else:
            fitted_curve = fitted_curve[:min_len]
            target_curve = target_curve[:min_len]
            error_norm = np.linalg.norm(fitted_curve - target_curve)
            relative_error = error_norm / (np.linalg.norm(target_curve) + 1e-6)

        # è®¡ç®—å¥–åŠ±
        # åœ¨stepæ–¹æ³•ä¸­ä¿®æ”¹å¥–åŠ±è®¡ç®—ï¼š
        base_reward = np.exp(10 / relative_error)  # æ›´æ•æ„Ÿ
        # ç²¾åº¦å¥–åŠ±
        if relative_error < 0.01:
            precision_bonus = 10.0
        elif relative_error < 0.05:
            precision_bonus = 5.0
        elif relative_error < 0.1:
            precision_bonus = 1.0
        else:
            precision_bonus = 0.0

        reward = base_reward + precision_bonus - 0.01
        done = True
        if hit_boundary:
            done = True
            reward -= 1.0

        if relative_error < 0.05:
            done = True
            reward += 50.0
            self.success_count += 1

        next_state = self._get_state()

        info = {
            'error_norm': error_norm,
            'relative_error': relative_error,
            'current_params': self.current_params.copy(),
            'target_params': self.current_target['params'] if self.current_target else None,
            'target_idx': self.current_target_idx,
            'hit_boundary': hit_boundary
        }

        return next_state, reward, done, info


# ------------------- æ”¹è¿›çš„ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº -------------------
class PrioritizedReplayBufferOptimized:
    """
    ä¼˜åŒ–ç‰ˆçš„ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
    - æ”¹è¿›äº†å†…å­˜ç®¡ç†
    - å¢å¼ºäº†æ•°æ®éªŒè¯
    - ä¼˜åŒ–äº†åºåˆ—åŒ–æ€§èƒ½
    """

    def __init__(self, capacity=50000, alpha=0.6, beta=0.4, beta_increment=0.001):
        self.capacity = int(capacity)
        self.alpha = alpha
        self.beta = beta
        self.beta_increment = beta_increment

        self.buffer = []
        self.priorities = np.zeros(self.capacity, dtype=np.float64)
        self.position = 0
        self.size = 0
        self.target_counts = {}

    def add(self, experience, error=None):
        """
        æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº

        å‚æ•°:
            experience: (state, action, reward, next_state, done, info)
            error: TDè¯¯å·®ï¼Œç”¨äºè®¡ç®—ä¼˜å…ˆçº§
        """
        # æ ‡å‡†åŒ–ç»éªŒæ•°æ®ç±»å‹
        state, action, reward, next_state, done, info = experience

        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
        state = np.array(state, dtype=np.float32)
        action = np.array(action, dtype=np.float32)
        next_state = np.array(next_state, dtype=np.float32)
        reward = float(reward)
        done = bool(done)

        # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
        if np.any(np.isnan(state)) or np.any(np.isnan(action)) or np.any(np.isnan(next_state)):
            logger.warning("æ£€æµ‹åˆ°NaNå€¼ï¼Œè·³è¿‡æ·»åŠ åˆ°buffer")
            return

        exp = (state, action, reward, next_state, done, info)

        # æ›´æ–°ç›®æ ‡è®¡æ•°
        target_idx = info.get('target_idx', None) if isinstance(info, dict) else None
        if target_idx is not None:
            self.target_counts[target_idx] = self.target_counts.get(target_idx, 0) + 1

        # è®¡ç®—ä¼˜å…ˆçº§
        if error is None:
            priority = self.priorities[:self.size].max() if self.size > 0 else 1.0
        else:
            priority = (abs(float(error)) + 1e-6) ** self.alpha

        # æ·»åŠ æˆ–æ›¿æ¢ç»éªŒ
        if self.size < self.capacity:
            self.buffer.append(exp)
        else:
            # æ›´æ–°ç›®æ ‡è®¡æ•°ï¼ˆç§»é™¤æ—§ç»éªŒçš„è®¡æ•°ï¼‰
            old_info = self.buffer[self.position][5]
            old_tidx = old_info.get('target_idx', None) if isinstance(old_info, dict) else None
            if old_tidx is not None and old_tidx in self.target_counts:
                self.target_counts[old_tidx] = max(0, self.target_counts[old_tidx] - 1)
            self.buffer[self.position] = exp

        self.priorities[self.position] = priority
        self.position = (self.position + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)

    def sample(self, batch_size):
        """
        é‡‡æ ·ä¸€æ‰¹ç»éªŒ

        å‚æ•°:
            batch_size: æ‰¹é‡å¤§å°

        è¿”å›:
            tuple: (states, actions, rewards, next_states, dones, indices, weights)
        """
        if self.size == 0:
            return None

        # è·å–ä¼˜å…ˆçº§å¹¶è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = self.priorities[:self.size].copy()
        pri_sum = priorities.sum()

        if pri_sum <= 0 or np.isnan(pri_sum):
            # å¦‚æœä¼˜å…ˆçº§æ— æ•ˆï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            probs = np.ones(self.size) / self.size
        else:
            probs = priorities / pri_sum

        # é‡‡æ ·ç´¢å¼•
        try:
            indices = np.random.choice(self.size, batch_size, p=probs, replace=True)
        except Exception as e:
            logger.warning(f"é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·: {e}")
            indices = np.random.choice(self.size, batch_size, replace=True)
            probs = np.ones(self.size) / self.size

        # æå–ç»éªŒ
        experiences = [self.buffer[idx] for idx in indices]

        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)

        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = (self.size * probs[indices]) ** (-self.beta)
        if np.max(weights) == 0:
            weights = np.ones_like(weights)
        else:
            weights = weights / weights.max()

        # è½¬æ¢ä¸ºå¼ é‡
        try:
            states_np = np.array([e[0] for e in experiences], dtype=np.float32)
            actions_np = np.array([e[1] for e in experiences], dtype=np.float32)
            rewards_np = np.array([[e[2]] for e in experiences], dtype=np.float32)
            next_states_np = np.array([e[3] for e in experiences], dtype=np.float32)
            dones_np = np.array([[1.0 if e[4] else 0.0] for e in experiences], dtype=np.float32)

            states = torch.from_numpy(states_np)
            actions = torch.from_numpy(actions_np)
            rewards = torch.from_numpy(rewards_np)
            next_states = torch.from_numpy(next_states_np)
            dones = torch.from_numpy(dones_np)
            weights = torch.FloatTensor(weights).unsqueeze(1)
        except Exception as e:
            logger.error(f"å¼ é‡è½¬æ¢å¤±è´¥: {e}")
            return None

        return states, actions, rewards, next_states, dones, indices, weights

    def update_priorities(self, indices, errors):
        """æ›´æ–°æŒ‡å®šç´¢å¼•çš„ä¼˜å…ˆçº§"""
        for idx, err in zip(indices, errors):
            if 0 <= idx < self.size:
                self.priorities[idx] = (abs(float(err)) + 1e-6) ** self.alpha

    def to_serializable(self):
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        serial = {
            'capacity': self.capacity,
            'alpha': self.alpha,
            'beta': self.beta,
            'beta_increment': self.beta_increment,
            'position': self.position,
            'size': self.size,
            'priorities': self.priorities[:self.size].tolist(),
            'buffer': [],
            'target_counts': self.target_counts
        }

        # åºåˆ—åŒ–bufferå†…å®¹
        for exp in self.buffer[:self.size]:
            state, action, reward, next_state, done, info = exp
            serial['buffer'].append((
                state.tolist(),
                action.tolist(),
                reward,
                next_state.tolist(),
                bool(done),
                info
            ))

        return serial

    @classmethod
    def from_serializable(cls, serial):
        """ä»åºåˆ—åŒ–æ ¼å¼æ¢å¤å¯¹è±¡"""
        obj = cls(
            capacity=serial.get('capacity', 50000),
            alpha=serial.get('alpha', 0.6),
            beta=serial.get('beta', 0.4),
            beta_increment=serial.get('beta_increment', 0.001)
        )

        obj.position = serial.get('position', 0)
        obj.size = serial.get('size', 0)

        # æ¢å¤ä¼˜å…ˆçº§
        priorities = np.array(serial.get('priorities', []), dtype=np.float64)
        obj.priorities = np.zeros(obj.capacity, dtype=np.float64)
        obj.priorities[:len(priorities)] = priorities

        # æ¢å¤bufferå†…å®¹
        obj.buffer = []
        for item in serial.get('buffer', []):
            s, a, r, ns, d, info = item
            s = np.array(s, dtype=np.float32)
            a = np.array(a, dtype=np.float32)
            ns = np.array(ns, dtype=np.float32)
            obj.buffer.append((s, a, r, ns, d, info))

        obj.target_counts = serial.get('target_counts', {})
        return obj

    def save(self, filename, compress=True):
        """ä¿å­˜bufferåˆ°æ–‡ä»¶"""
        try:
            serial = self.to_serializable()
            if compress:
                with gzip.open(filename, 'wb') as f:
                    pickle.dump(serial, f, protocol=pickle.HIGHEST_PROTOCOL)
            else:
                with open(filename, 'wb') as f:
                    pickle.dump(serial, f, protocol=pickle.HIGHEST_PROTOCOL)
            logger.info(f"Replay bufferä¿å­˜æˆåŠŸ: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜replay bufferå¤±è´¥: {e}")

    @staticmethod
    def load(filename, compress=True):
        """ä»æ–‡ä»¶åŠ è½½buffer"""
        try:
            if compress:
                with gzip.open(filename, 'rb') as f:
                    serial = pickle.load(f)
            else:
                with open(filename, 'rb') as f:
                    serial = pickle.load(f)

            buf = PrioritizedReplayBufferOptimized.from_serializable(serial)
            logger.info(f"Replay bufferåŠ è½½æˆåŠŸ: {filename}, size={buf.size}")
            return buf
        except Exception as e:
            logger.error(f"åŠ è½½replay bufferå¤±è´¥: {e}")
            return None


# ------------------- Actor/Critic ç½‘ç»œå®šä¹‰ -------------------
class UniversalActor(nn.Module):
    """é€šç”¨Actorç½‘ç»œ"""

    def __init__(self, state_dim=43, action_dim=3):  # ä»63å‡å°‘åˆ°43ï¼ˆ30+3+10ï¼‰
        super().__init__()

        # ç‰¹å¾æå–å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.01),
        )

        # åŠ¨ä½œè¾“å‡ºå±‚
        self.action_layers = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.LeakyReLU(0.01),

            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.LeakyReLU(0.01),

            nn.Linear(64, action_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
        )

        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        features = self.feature_layers(state)
        action = self.action_layers(features)
        return action


class UniversalCritic(nn.Module):
    """é€šç”¨Criticç½‘ç»œ"""

    def __init__(self, state_dim=43, action_dim=3):
        super().__init__()

        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.01),
        )

        # åŠ¨ä½œç¼–ç å™¨
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, 64),
            nn.LayerNorm(64),
            nn.LeakyReLU(0.01),
        )

        # Qå€¼è®¡ç®—å±‚
        self.q_layers = nn.Sequential(
            nn.Linear(256 + 64, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.LeakyReLU(0.01),

            nn.Linear(64, 1)
        )

        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, state, action):
        """å‰å‘ä¼ æ’­"""
        state_features = self.state_encoder(state)
        action_features = self.action_encoder(action)
        combined = torch.cat([state_features, action_features], dim=1)
        q_value = self.q_layers(combined)
        return q_value


# ------------------- æ¨¡å‹ä¿å­˜/åŠ è½½å·¥å…· -------------------
def save_model_atomic(save_path, actor, critic, optimizer_actor=None, optimizer_critic=None, extra=None):
    """ä¿®å¤ç‰ˆçš„æ¨¡å‹ä¿å­˜å‡½æ•°"""

    # ğŸ”§ åœ¨åŒä¸€ç›®å½•ä¸‹åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    save_dir = os.path.dirname(os.path.abspath(save_path)) or '.'
    os.makedirs(save_dir, exist_ok=True)

    tmp_fd, tmp_path = tempfile.mkstemp(
        suffix='.tmp',
        prefix='tmp_model_',
        dir=save_dir  # å…³é”®ä¿®å¤ï¼šåŒä¸€ç›®å½•
    )
    os.close(tmp_fd)

    try:
        payload = {
            'actor_state_dict': actor.state_dict(),
            'critic_state_dict': critic.state_dict()
        }

        if optimizer_actor is not None:
            payload['optimizer_actor_state_dict'] = optimizer_actor.state_dict()
        if optimizer_critic is not None:
            payload['optimizer_critic_state_dict'] = optimizer_critic.state_dict()
        if extra is not None:
            payload['extra'] = extra

        torch.save(payload, tmp_path)

        # ğŸ”§ Windowså…¼å®¹çš„æ–‡ä»¶æ›¿æ¢
        abs_save_path = os.path.abspath(save_path)
        if os.path.exists(abs_save_path):
            os.remove(abs_save_path)
        shutil.move(tmp_path, abs_save_path)

        logger.info(f"æ¨¡å‹ä¿å­˜æˆåŠŸ: {save_path}")

    finally:
        if os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except:
                pass


def load_model_if_exists(path, actor, critic, optimizer_actor=None, optimizer_critic=None, map_location='cpu'):
    """
    å¦‚æœæ¨¡å‹æ–‡ä»¶å­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™è¿”å›None
    """
    if not os.path.exists(path):
        logger.info(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return None

    try:
        data = torch.load(path, map_location=map_location)

        if 'actor_state_dict' in data:
            actor.load_state_dict(data['actor_state_dict'])
        if 'critic_state_dict' in data:
            critic.load_state_dict(data['critic_state_dict'])
        if optimizer_actor is not None and 'optimizer_actor_state_dict' in data:
            optimizer_actor.load_state_dict(data['optimizer_actor_state_dict'])
        if optimizer_critic is not None and 'optimizer_critic_state_dict' in data:
            optimizer_critic.load_state_dict(data['optimizer_critic_state_dict'])

        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {path}")
        return data.get('extra', None)

    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None


# ------------------- è½¯æ›´æ–°å‡½æ•° -------------------
def soft_update(target_net, source_net, tau=0.005):
    """
    è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°
    target = tau * source + (1 - tau) * target
    """
    for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + source_param.data * tau)


# ------------------- ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•° -------------------
def train_universal_ddpg_optimized(env, actor, critic, target_data, config):
    """
    ä¼˜åŒ–ç‰ˆçš„DDPGè®­ç»ƒå‡½æ•°
    - å†…å­˜ä½¿ç”¨ä¼˜åŒ–
    - æ”¹è¿›çš„å¤šè¿›ç¨‹æ”¯æŒ
    - å¢å¼ºçš„é”™è¯¯å¤„ç†
    - ç³»ç»Ÿèµ„æºç›‘æ§
    """

    # è§£åŒ…é…ç½®å‚æ•°
    num_episodes = config.get('num_episodes', 20000)
    max_steps = config.get('max_steps', 25)
    batch_size = config.get('batch_size', 64)
    gamma = config.get('gamma', 0.98)
    actor_lr = config.get('actor_lr', 1e-4)
    critic_lr = config.get('critic_lr', 1e-3)
    tau = config.get('tau', 0.005)
    buffer_size = config.get('buffer_size', 50000)
    n_parallel = config.get('n_parallel', 2)
    num_workers = config.get('num_workers', 4)
    use_multiprocessing = config.get('use_multiprocessing', True)
    save_every = config.get('save_every', 100)
    buffer_save_path = config.get('buffer_save_path', 'replay_buffer.pkl.gz')
    model_path = config.get('model_path', 'steel01_model.pth')

    logger.info("å¼€å§‹è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        logger.info(f"  {key}: {value}")

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    actor_optimizer = optim.AdamW(actor.parameters(), lr=actor_lr)
    critic_optimizer = optim.AdamW(critic.parameters(), lr=critic_lr)

    # åˆ›å»ºç›®æ ‡ç½‘ç»œ
    target_actor = UniversalActor(state_dim=43, action_dim=3)
    target_critic = UniversalCritic(state_dim=43, action_dim=3)
    target_actor.load_state_dict(actor.state_dict())
    target_critic.load_state_dict(critic.state_dict())
    target_actor.eval()
    target_critic.eval()

    # åŠ è½½å·²æœ‰æ¨¡å‹å’Œbuffer
    extra = load_model_if_exists(model_path, actor, critic, actor_optimizer, critic_optimizer)
    start_episode = 0
    if extra and 'episode' in extra:
        start_episode = extra['episode']
        logger.info(f"ä»episode {start_episode}ç»§ç»­è®­ç»ƒ")

    # åˆå§‹åŒ–replay buffer
    buffer = None
    if os.path.exists(buffer_save_path):
        buffer = PrioritizedReplayBufferOptimized.load(buffer_save_path, compress=True)

    if buffer is None:
        buffer = PrioritizedReplayBufferOptimized(capacity=buffer_size)
        logger.info(f"åˆ›å»ºæ–°çš„replay bufferï¼Œå®¹é‡: {buffer_size}")

    # è®­ç»ƒç»Ÿè®¡
    training_stats = {
        'episode_rewards': [],
        'episode_errors': [],
        'success_rate': deque(maxlen=100),
        'actor_losses': [],
        'critic_losses': [],
        'noise_levels': [],
        'opensees_failure_rate': [],
        'memory_usage': []
    }

    logger.info("å¼€å§‹è®­ç»ƒ...")

    # å¤šè¿›ç¨‹æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    pool = None
    if use_multiprocessing and num_workers > 1:
        try:
            pool = ProcessPoolExecutor(max_workers=num_workers)
            logger.info(f"å¯ç”¨å¤šè¿›ç¨‹æ± ï¼Œworkeræ•°é‡: {num_workers}")
        except Exception as e:
            logger.warning(f"åˆ›å»ºè¿›ç¨‹æ± å¤±è´¥ï¼Œå°†ä½¿ç”¨å•è¿›ç¨‹: {e}")
            use_multiprocessing = False

    try:
        # ä¸»è®­ç»ƒå¾ªç¯
        for episode in range(start_episode, num_episodes):
            # é‡ç½®ç¯å¢ƒ
            state = env.reset()
            episode_reward = 0.0
            episode_errors = []

            # å™ªå£°æ°´å¹³ï¼ˆéšè®­ç»ƒé€æ¸å‡å°‘ï¼‰
            noise_level = 0.2 * (0.99 ** (episode // 100))

            # episodeå†…çš„æ­¥éª¤å¾ªç¯
            for step in range(max_steps):
                # ä½¿ç”¨actorç½‘ç»œé€‰æ‹©ä¸»è¦åŠ¨ä½œ
                with torch.no_grad():
                    s_t = torch.FloatTensor(state).unsqueeze(0)
                    action_main = actor(s_t).cpu().numpy()[0]

                # ç”Ÿæˆå¤šä¸ªå€™é€‰åŠ¨ä½œï¼ˆåŒ…æ‹¬ä¸»åŠ¨ä½œï¼‰
                actions = [action_main]
                for _ in range(n_parallel - 1):
                    noise = np.random.normal(0, noise_level, size=action_main.shape)
                    a = np.clip(action_main + noise, -1, 1)
                    actions.append(a)
                actions = np.array(actions)

                # è®¡ç®—æ¯ä¸ªåŠ¨ä½œå¯¹åº”çš„æ–°å‚æ•°
                base_params = env.current_params.copy()
                params_list = []
                for a in actions:
                    new_p = []
                    for i, (p, ai) in enumerate(zip(base_params, a)):
                        delta = env.scale_factors[i] * ai
                        new_val = float(np.clip(p + delta, env.param_ranges[i][0], env.param_ranges[i][1]))
                        new_p.append(new_val)
                    params_list.append(new_p)

                # å¹¶è¡Œè®¡ç®—æ»å›æ›²çº¿
                if use_multiprocessing and pool is not None and num_workers > 1:
                    # å¤šè¿›ç¨‹ç‰ˆæœ¬
                    try:
                        # å°†å‚æ•°åˆ—è¡¨åˆ†å—
                        chunks = []
                        k = min(num_workers, len(params_list))
                        chunk_size = max(1, (len(params_list) + k - 1) // k)
                        for i in range(0, len(params_list), chunk_size):
                            chunks.append(params_list[i:i + chunk_size])

                        # æäº¤ä»»åŠ¡
                        protocol = env.current_target['protocol'] if env.current_target else np.zeros(100)
                        futures = []
                        for ch in chunks:
                            futures.append(pool.submit(worker_hysteretic_batch_improved, (ch, protocol)))

                        # æ”¶é›†ç»“æœ
                        results_chunks = []
                        for fut in as_completed(futures):
                            try:
                                res = fut.result(timeout=30)  # 30ç§’è¶…æ—¶
                                results_chunks.append(res)
                            except Exception as e:
                                logger.warning(f"Workerä»»åŠ¡å¤±è´¥: {e}")
                                res = np.zeros((len(chunks[0]), len(protocol)))
                                results_chunks.append(res)

                        # åˆå¹¶ç»“æœ
                        batch_forces = np.vstack(results_chunks)

                    except Exception as e:
                        logger.warning(f"å¤šè¿›ç¨‹è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°å•è¿›ç¨‹: {e}")
                        protocol = env.current_target['protocol'] if env.current_target else np.zeros(100)
                        batch_forces = env.hysteretic_curve_batch(params_list, protocol=protocol)
                else:
                    # å•è¿›ç¨‹ç‰ˆæœ¬
                    protocol = env.current_target['protocol'] if env.current_target else np.zeros(100)
                    batch_forces = env.hysteretic_curve_batch(params_list, protocol=protocol)

                # è®¡ç®—å¥–åŠ±å’Œç»éªŒ
                done_flag = False
                for idx_par, (new_params, fitted_curve, act) in enumerate(zip(params_list, batch_forces, actions)):
                    # è®¡ç®—è¯¯å·®
                    target_curve = env.current_target['curve'] if env.current_target else np.zeros_like(fitted_curve)
                    min_len = min(len(fitted_curve), len(target_curve))

                    if min_len == 0:
                        error_norm = float('inf')
                        relative_error = 1.0
                    else:
                        fitted_cut = fitted_curve[:min_len]
                        target_cut = target_curve[:min_len]
                        error_norm = np.linalg.norm(fitted_cut - target_cut)
                        relative_error = error_norm / (np.linalg.norm(target_cut) + 1e-6)

                    # è®¡ç®—å¥–åŠ±
                    base_reward = np.exp(10 / relative_error)
                    # ç²¾åº¦å¥–åŠ±
                    if relative_error < 0.01:
                        precision_bonus = 10.0
                    elif relative_error < 0.05:
                        precision_bonus = 5.0
                    elif relative_error < 0.1:
                        precision_bonus = 1.0
                    else:
                        precision_bonus = 0.0

                    reward = base_reward + precision_bonus - 0.01
                    # æ£€æŸ¥è¾¹ç•Œç¢°æ’
                    hit_boundary = any([
                        (new_params[i] == env.param_ranges[i][0]) or
                        (new_params[i] == env.param_ranges[i][1])
                        for i in range(len(new_params))
                    ])

                    done = False
                    if hit_boundary:
                        reward -= 1.0
                        done = True
                    if relative_error < 0.05:
                        reward += 50.0
                        done = True

                    # è®¡ç®—ä¸‹ä¸€çŠ¶æ€
                    oldp = env.current_params
                    env.current_params = new_params
                    next_state = env._get_state()
                    env.current_params = oldp

                    # å‡†å¤‡ç»éªŒæ•°æ®
                    state_arr = np.array(state, dtype=np.float32)
                    act_arr = np.array(act, dtype=np.float32)
                    next_state_arr = np.array(next_state, dtype=np.float32)
                    exp_info = {
                        'error_norm': error_norm,
                        'relative_error': relative_error,
                        'current_params': new_params.copy(),
                        'target_idx': env.current_target_idx,
                        'hit_boundary': hit_boundary
                    }

                    experience = (state_arr, act_arr, float(reward), next_state_arr, bool(done), exp_info)

                    # æ·»åŠ åˆ°buffer
                    buffer.add(experience, error=relative_error)

                    # ä¸»è·¯å¾„ï¼ˆç¬¬ä¸€ä¸ªå€™é€‰ï¼‰æ›´æ–°ç¯å¢ƒçŠ¶æ€
                    if idx_par == 0:
                        env.current_params = new_params
                        state = next_state
                        episode_reward += reward
                        if 'relative_error' in exp_info:
                            episode_errors.append(exp_info['relative_error'])
                        if done:
                            done_flag = True
                            break

                # å¦‚æœä¸»è·¯å¾„å®Œæˆï¼Œè·³å‡ºæ­¥éª¤å¾ªç¯
                if done_flag:
                    break

                # è¿›è¡Œå­¦ä¹ æ›´æ–°ï¼ˆå½“bufferæœ‰è¶³å¤Ÿæ•°æ®æ—¶ï¼‰
                if buffer.size > max(batch_size, 256):
                    for _ in range(3):  # å‡å°‘åˆ°3æ¬¡æ›´æ–°
                        batch = buffer.sample(batch_size)
                        if batch is None:
                            continue

                        states_b, actions_b, rewards_b, next_states_b, dones_b, indices_b, weights_b = batch

                        # Criticæ›´æ–°
                        with torch.no_grad():
                            next_actions = target_actor(next_states_b)
                            next_q_values = target_critic(next_states_b, next_actions)
                            target_q = rewards_b + gamma * next_q_values * (1 - dones_b)

                        current_q = critic(states_b, actions_b)
                        td_errors = (target_q - current_q).detach().cpu().numpy().flatten()

                        critic_loss = (weights_b * (current_q - target_q) ** 2).mean()
                        critic_optimizer.zero_grad()
                        critic_loss.backward()
                        torch.nn.utils.clip_grad_norm_(critic.parameters(), 5.0)
                        critic_optimizer.step()

                        # Actoræ›´æ–°
                        actor_actions = actor(states_b)
                        actor_loss = -(critic(states_b, actor_actions) * weights_b).mean()
                        actor_optimizer.zero_grad()
                        actor_loss.backward()
                        torch.nn.utils.clip_grad_norm_(actor.parameters(), 5.0)
                        actor_optimizer.step()

                        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
                        soft_update(target_actor, actor, tau)
                        soft_update(target_critic, critic, tau)

                        # æ›´æ–°bufferä¼˜å…ˆçº§
                        buffer.update_priorities(indices_b, td_errors)

                        # è®°å½•æŸå¤±
                        training_stats['actor_losses'].append(actor_loss.item())
                        training_stats['critic_losses'].append(critic_loss.item())

            # episodeç»“æŸï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
            training_stats['episode_rewards'].append(episode_reward)
            training_stats['episode_errors'].append(np.mean(episode_errors) if episode_errors else np.nan)
            training_stats['success_rate'].append(1 if (episode_errors and episode_errors[-1] < 0.01) else 0)
            training_stats['noise_levels'].append(noise_level)

            # OpenSeeså¤±è´¥ç‡
            op_total = env.opensees_total_count
            op_fail = env.opensees_failure_count
            failure_rate = op_fail / op_total if op_total > 0 else 0.0
            training_stats['opensees_failure_rate'].append(failure_rate)

            # ç³»ç»Ÿèµ„æºç›‘æ§
            if episode % 100 == 0:
                resources = monitor_system_resources()
                if resources:
                    training_stats['memory_usage'].append(resources['memory_percent'])

                    # å†…å­˜è¿‡é«˜æ—¶è¿›è¡Œåƒåœ¾å›æ”¶
                    if resources['memory_percent'] > 80:
                        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶")
                        force_garbage_collection()

            # å®šæœŸæ—¥å¿—è¾“å‡º
            if episode % 10 == 0:
                pr = buffer.priorities[:buffer.size] if buffer.size > 0 else np.array([0.])
                logger.info(f"Ep {episode}/{num_episodes} | "
                            f"reward={episode_reward:.4f} | "
                            f"buffer_size={buffer.size} | "
                            f"priorities min/max/mean={pr.min():.3e}/{pr.max():.3e}/{pr.mean():.3e} | "
                            f"opensees_fail_rate={failure_rate:.3%}")

            # å®šæœŸä¿å­˜
            if episode % save_every == 0 and episode > 0:
                extra = {
                    'episode': episode,
                    'stats_snapshot': {
                        k: (v[-1] if isinstance(v, list) and v else None)
                        for k, v in training_stats.items()
                    }
                }
                try:
                    save_model_atomic(model_path, actor, critic, actor_optimizer, critic_optimizer, extra=extra)
                except Exception as save_error:
                    logger.warning(f"ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {save_error}")
                    # å¤‡ç”¨ä¿å­˜æ–¹æ³•ï¼šç›´æ¥ä¿å­˜ï¼Œä¸ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
                    torch.save({
                        'actor_state_dict': actor.state_dict(),
                        'critic_state_dict': critic.state_dict(),
                        'optimizer_actor_state_dict': actor_optimizer.state_dict(),
                        'optimizer_critic_state_dict': critic_optimizer.state_dict(),
                        'extra': extra
                    }, model_path)
                    logger.info(f"å¤‡ç”¨ä¿å­˜æˆåŠŸ: {model_path}")
                try:
                    buffer.save(buffer_save_path, compress=True)
                except Exception as e:
                    logger.error(f"ä¿å­˜bufferå¤±è´¥: {e}")

            # è¯¦ç»†è¯Šæ–­ï¼ˆæ¯100ä¸ªepisodeï¼‰
            if episode % 100 == 0 and episode > 0:
                if training_stats['critic_losses']:
                    recent_critic_loss = np.mean(training_stats['critic_losses'][-50:])
                    logger.info(f"æœ€è¿‘critic_losså‡å€¼: {recent_critic_loss:.6f}")
                logger.info(f"OpenSees ç»Ÿè®¡ - æ€»è®¡/å¤±è´¥: {op_total}/{op_fail} (å¤±è´¥ç‡={failure_rate:.3%})")

                # æˆåŠŸç‡ç»Ÿè®¡
                if training_stats['success_rate']:
                    recent_success_rate = np.mean(list(training_stats['success_rate']))
                    logger.info(f"æœ€è¿‘100ä¸ªepisodeæˆåŠŸç‡: {recent_success_rate:.3%}")

    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.exception(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if pool is not None:
            logger.info("å…³é—­è¿›ç¨‹æ± ...")
            pool.shutdown(wait=True)

        # æœ€ç»ˆä¿å­˜
        logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œbuffer...")
        extra = {'episode': episode if 'episode' in locals() else num_episodes, 'finished': True}
        try:
            save_model_atomic(model_path, actor, critic, actor_optimizer, critic_optimizer, extra=extra)
        except Exception as save_error:
            logger.warning(f"ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {save_error}")
            # å¤‡ç”¨ä¿å­˜æ–¹æ³•ï¼šç›´æ¥ä¿å­˜ï¼Œä¸ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            torch.save({
                'actor_state_dict': actor.state_dict(),
                'critic_state_dict': critic.state_dict(),
                'optimizer_actor_state_dict': actor_optimizer.state_dict(),
                'optimizer_critic_state_dict': critic_optimizer.state_dict(),
                'extra': extra
            }, model_path)
            logger.info(f"å¤‡ç”¨ä¿å­˜æˆåŠŸ: {model_path}")
        try:
            buffer.save(buffer_save_path, compress=True)
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ç»ˆbufferå¤±è´¥: {e}")

    return training_stats


# ------------------- å‚æ•°è¯†åˆ«å‡½æ•° -------------------
def identify_curve_parameters(actor, target_curve, protocol,
                              param_ranges=None, scale_factors=None,
                              max_iterations=30, tolerance=0.01, verbose=True):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„actorç½‘ç»œè¯†åˆ«æ›²çº¿å‚æ•°

    å‚æ•°:
        actor: è®­ç»ƒå¥½çš„actorç½‘ç»œ
        target_curve: ç›®æ ‡åŠ›æ›²çº¿
        protocol: ä½ç§»å†ç¨‹
        param_ranges: å‚æ•°èŒƒå›´
        scale_factors: å‚æ•°è°ƒæ•´æ¯”ä¾‹
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        tolerance: æ”¶æ•›å®¹å·®
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    è¿”å›:
        list: è¯†åˆ«å‡ºçš„å‚æ•° [fy, E, b]
    """
    if param_ranges is None:
        param_ranges = [[1, 1000], [1, 50000], [0.01, 0.3]]
    if scale_factors is None:
        scale_factors = [10, 500, 0.005]

    # åˆ›å»ºä¸´æ—¶ç¯å¢ƒ
    temp_env = MultiTargetHysteresisEnvOptimized([])
    temp_env.param_ranges = param_ranges
    temp_env.scale_factors = scale_factors
    temp_env.current_target = {
        'curve': target_curve,
        'protocol': protocol,
        'params': [0, 0, 0]
    }

    # åˆå§‹å‚æ•°ï¼ˆä½¿ç”¨èŒƒå›´ä¸­ç‚¹ï¼‰
    current_params = [(r[0] + r[1]) / 2 for r in param_ranges]

    actor.eval()

    logger.info("å¼€å§‹å‚æ•°è¯†åˆ«...")

    for iteration in range(max_iterations):
        # è®¾ç½®å½“å‰å‚æ•°å¹¶è·å–çŠ¶æ€
        temp_env.current_params = current_params
        state = temp_env._get_state()

        # ä½¿ç”¨actorç½‘ç»œé¢„æµ‹åŠ¨ä½œ
        with torch.no_grad():
            action = actor(torch.FloatTensor(state).unsqueeze(0)).cpu().numpy()[0]

        # æ›´æ–°å‚æ•°
        new_params = []
        for i, (p, a) in enumerate(zip(current_params, action)):
            new_p = p + scale_factors[i] * a
            new_p = np.clip(new_p, param_ranges[i][0], param_ranges[i][1])
            new_params.append(new_p)

        # è®¡ç®—æ‹Ÿåˆæ›²çº¿å’Œè¯¯å·®
        fitted_curve = temp_env.hysteretic_curve_single(new_params, protocol)
        error = np.linalg.norm(fitted_curve - target_curve)
        relative_error = error / (np.linalg.norm(target_curve) + 1e-6)

        if verbose:
            logger.info(f"è¿­ä»£ {iteration + 1}: "
                        f"params=[{new_params[0]:.2f}, {new_params[1]:.2f}, {new_params[2]:.4f}] "
                        f"ç›¸å¯¹è¯¯å·®={relative_error:.6f}")

        # æ£€æŸ¥æ”¶æ•›
        if relative_error < tolerance:
            logger.info(f"å‚æ•°è¯†åˆ«æ”¶æ•›! ç›¸å¯¹è¯¯å·®={relative_error:.6f}")
            break

        current_params = new_params

    return current_params


# ------------------- è®­ç»ƒè¿›åº¦å¯è§†åŒ– -------------------
def plot_training_progress(stats, save_path='training_progress.png'):
    """
    ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾è¡¨

    å‚æ•°:
        stats: è®­ç»ƒç»Ÿè®¡æ•°æ®
        save_path: ä¿å­˜è·¯å¾„
    """
    try:
        plt.style.use('seaborn-v0_8-darkgrid')
    except:
        plt.style.use('default')

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # å¥–åŠ±æ›²çº¿
    ax = axes[0, 0]
    rewards = stats['episode_rewards']
    if rewards:
        ax.plot(rewards, alpha=0.3, label='Raw Reward', color='blue')
        if len(rewards) > 100:
            window = min(100, len(rewards) // 10)
            ma = np.convolve(rewards, np.ones(window) / window, mode='valid')
            ax.plot(range(window - 1, len(rewards)), ma, label=f'{window}-Episode MA', color='red', linewidth=2)
    ax.set_title('Episode Rewards')  # è‹±æ–‡æ ‡é¢˜
    ax.set_xlabel('Episode')
    ax.set_ylabel('Total Reward')
    ax.legend()
    ax.grid(True)

    # è¯¯å·®æ›²çº¿
    ax = axes[0, 1]
    if stats['episode_errors']:
        errs = np.array([e for e in stats['episode_errors'] if not np.isnan(e)], dtype=np.float64)
        if len(errs) > 0:
            ax.plot(errs, alpha=0.6, color='orange')
            ax.set_title('Episode Mean Relative Error')
            ax.set_xlabel('Episode')
            ax.set_ylabel('Relative Error')
            ax.set_yscale('log')
            ax.grid(True)

    # æˆåŠŸç‡
    ax = axes[0, 2]
    if stats['success_rate']:
        sr = list(stats['success_rate'])
        x = range(len(sr))
        ax.plot(x, sr, alpha=0.7, color='green')
        ax.set_title('Success Rate (Sliding Window)')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Success Rate')
        ax.set_ylim(0, 1.05)
        ax.grid(True)

    # æŸå¤±æ›²çº¿
    ax = axes[1, 0]
    if stats['actor_losses'] and stats['critic_losses']:
        actor_losses = stats['actor_losses']
        critic_losses = stats['critic_losses']
        ax.plot(actor_losses, alpha=0.6, label='Actor Loss', color='red')
        ax.plot(critic_losses, alpha=0.6, label='Critic Loss', color='blue')
        ax.set_title('Network Losses')
        ax.set_xlabel('Update Steps')
        ax.set_ylabel('Loss Value')
        ax.legend()
        ax.grid(True)

    # OpenSeeså¤±è´¥ç‡
    ax = axes[1, 1]
    if stats.get('opensees_failure_rate'):
        fail_rates = stats['opensees_failure_rate']
        ax.plot(fail_rates, alpha=0.7, color='purple')
        ax.set_title('OpenSees Failure Rate')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Failure Rate')
        ax.grid(True)

    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    ax = axes[1, 2]
    if stats.get('memory_usage'):
        memory = stats['memory_usage']
        ax.plot(memory, alpha=0.7, color='brown')
        ax.set_title('Memory Usage')
        ax.set_xlabel('Monitoring Point')
        ax.set_ylabel('Memory Usage (%)')
        ax.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Warning Line (80%)')
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    logger.info(f"Training progress saved to: {save_path}")


# ------------------- è·å–ä¼˜åŒ–é…ç½® -------------------
# åœ¨mainå‡½æ•°ä¸­ï¼Œå°†é…ç½®æ”¹ä¸ºï¼š
def get_emergency_fast_config():
    return {
        'num_episodes': 1000,         # å…ˆæµ‹è¯•1000å›åˆ
        'max_steps': 10,              # å‡å°‘æ­¥æ•°
        'batch_size': 64,
        'actor_lr': 1e-3,             # ğŸ”§ å¢å¤§10å€
        'critic_lr': 5e-3,            # ğŸ”§ å¢å¤§5å€
        'tau': 0.02,                  # ğŸ”§ å¢å¤§4å€
        'buffer_size': 5000,          # ğŸ”§ å‡å°10å€
        'n_parallel': 1,              # ğŸ”§ å•å€™é€‰
        'num_workers': 1,             # ğŸ”§ å•è¿›ç¨‹
        'use_multiprocessing': False, # ğŸ”§ ç¦ç”¨å¤šè¿›ç¨‹
        'save_every': 50,
        'model_path': 'fast_test_model.pth'
    }


# ------------------- ä¸»å‡½æ•° -------------------
def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    txt_path = r"D:\Charles\PycharmProjects\PythonProject\.venv\vacation_task\final\data_files\new_data\10steel01data.txt"
    excel_path = r"D:\Charles\PycharmProjects\PythonProject\.venv\vacation_task\final\data_files\(250,15000,0.1)3åœˆsteel01.xlsx"

    logger.info("=" * 60)
    logger.info("Steel01 å‚æ•°è¯†åˆ«ç³»ç»Ÿ (ä¼˜åŒ–ç‰ˆ - 8æ ¸16GB)")
    logger.info("=" * 60)

    # ç³»ç»Ÿä¿¡æ¯
    if PSUTIL_AVAILABLE:
        logger.info("ç³»ç»Ÿé…ç½®ä¿¡æ¯:")
        logger.info(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
        logger.info(f"  æ€»å†…å­˜: {psutil.virtual_memory().total / (1024 ** 3):.1f} GB")
        logger.info(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024 ** 3):.1f} GB")

    # æ¨¡å¼é€‰æ‹©
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("[1] è®­ç»ƒæ–°æ¨¡å‹")
    print("[2] ä½¿ç”¨å·²æœ‰æ¨¡å‹è¿›è¡Œå‚æ•°è¯†åˆ«")
    print("[3] ç»§ç»­è®­ç»ƒå·²æœ‰æ¨¡å‹")

    mode = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

    if mode == '1' or mode == '3':
        # è®­ç»ƒæ¨¡å¼
        logger.info(f"{'è®­ç»ƒæ–°æ¨¡å‹' if mode == '1' else 'ç»§ç»­è®­ç»ƒå·²æœ‰æ¨¡å‹'}")

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if not os.path.exists(txt_path):
            logger.error(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
            logger.error("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return

        # åŠ è½½è®­ç»ƒæ•°æ®
        target_data = load_training_data_from_txt(txt_path)
        if not target_data:
            logger.error("æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
            return

        # åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œ
        env = MultiTargetHysteresisEnvOptimized(target_data, feature_dim=30)
        actor = UniversalActor(state_dim=43, action_dim=3)
        critic = UniversalCritic(state_dim=43, action_dim=3)

        # è·å–ä¼˜åŒ–é…ç½®
        config = get_emergency_fast_config()
        # config = get_test_config()

        # ç”¨æˆ·ç¡®è®¤é…ç½®
        print("\nè®­ç»ƒé…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")

        confirm = input("\næ˜¯å¦ä½¿ç”¨æ­¤é…ç½®å¼€å§‹è®­ç»ƒ? (y/n): ").strip().lower()
        if confirm != 'y':
            logger.info("è®­ç»ƒå–æ¶ˆ")
            return

        # å¼€å§‹è®­ç»ƒ
        try:
            training_stats = train_universal_ddpg_optimized(env, actor, critic, target_data, config)

            # ç»˜åˆ¶è®­ç»ƒè¿›åº¦
            plot_training_progress(training_stats)
            logger.info("è®­ç»ƒå®Œæˆ!")

        except Exception as e:
            logger.exception(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    elif mode == '2':
        # å‚æ•°è¯†åˆ«æ¨¡å¼
        logger.info("å‚æ•°è¯†åˆ«æ¨¡å¼")

        # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
        model_files = glob.glob('steel01_model*.pth')
        # model_files = glob.glob('test_model.pth')
        if not model_files:
            logger.error("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
            logger.error("è¯·å…ˆä½¿ç”¨æ¨¡å¼1è®­ç»ƒæ¨¡å‹")
            return

        # é€‰æ‹©æ¨¡å‹æ–‡ä»¶
        if len(model_files) == 1:
            model_path = model_files[0]
            logger.info(f"ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_path}")
        else:
            logger.info("å‘ç°å¤šä¸ªæ¨¡å‹æ–‡ä»¶:")
            for i, f in enumerate(model_files):
                logger.info(f"  {i + 1}. {f}")
            try:
                choice = int(input("è¯·é€‰æ‹©æ¨¡å‹æ–‡ä»¶ç¼–å·: ").strip()) - 1
                model_path = model_files[choice]
            except (ValueError, IndexError):
                logger.error("æ— æ•ˆé€‰æ‹©")
                return

        # åŠ è½½æ¨¡å‹
        actor = UniversalActor(state_dim=43, action_dim=3)
        critic = UniversalCritic(state_dim=43, action_dim=3)
        extra = load_model_if_exists(model_path, actor, critic)

        if extra is None:
            logger.error("æ¨¡å‹åŠ è½½å¤±è´¥")
            return

        # æ£€æŸ¥æµ‹è¯•æ•°æ®æ–‡ä»¶
        if not os.path.exists(excel_path):
            logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            logger.error("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return

        # åŠ è½½æµ‹è¯•æ•°æ®
        logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {excel_path}")
        displacement, force = load_test_data_from_excel(excel_path)

        if len(displacement) == 0 or len(force) == 0:
            logger.error("æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥")
            return

        logger.info(f"æµ‹è¯•æ•°æ®ç‚¹æ•°: {len(displacement)}")

        # å‚æ•°è¯†åˆ«
        logger.info("å¼€å§‹å‚æ•°è¯†åˆ«...")
        try:
            params = identify_curve_parameters(
                actor, force, displacement,
                max_iterations=50,
                tolerance=0.01,
                verbose=True
            )

            logger.info("=" * 40)
            logger.info("å‚æ•°è¯†åˆ«ç»“æœ:")
            logger.info(f"  å±ˆæœå¼ºåº¦ (fy): {params[0]:.2f}")
            logger.info(f"  å¼¹æ€§æ¨¡é‡ (E):  {params[1]:.2f}")
            logger.info(f"  åº”å˜ç¡¬åŒ–æ¯” (b): {params[2]:.4f}")
            logger.info("=" * 40)

            # éªŒè¯ç»“æœ
            env = MultiTargetHysteresisEnvOptimized([])
            env.current_target = {'protocol': displacement, 'curve': force}
            env.current_params = params
            predicted_force = env.hysteretic_curve_single(params, displacement)

            # è®¡ç®—è¯¯å·®
            error = np.linalg.norm(predicted_force - force)
            relative_error = error / (np.linalg.norm(force) + 1e-6)
            logger.info(f"éªŒè¯è¯¯å·®: ç›¸å¯¹è¯¯å·® = {relative_error:.6f}")

            # ç»˜åˆ¶å¯¹æ¯”å›¾
            plt.figure(figsize=(12, 8))
            plt.plot(displacement, force, 'b-', linewidth=2, label='target_curve', alpha=0.8)
            plt.plot(displacement, predicted_force, 'r--', linewidth=2, label='fitting_curve', alpha=0.8)
            plt.xlabel('displacement')
            plt.ylabel('force')
            plt.title(f'Steel01 comparison of parameter recognition results\n'
                      f'fy={params[0]:.2f}, E={params[1]:.2f}, b={params[2]:.4f}\n'
                      f'Relative error={relative_error:.6f}')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.show()

        except Exception as e:
            logger.exception(f"å‚æ•°è¯†åˆ«è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    else:
        logger.error("æ— æ•ˆçš„æ¨¡å¼é€‰æ‹©")


# ------------------- ç¨‹åºå…¥å£ -------------------
if __name__ == "__main__":
    # Windowså¤šè¿›ç¨‹æ”¯æŒ
    import multiprocessing

    multiprocessing.freeze_support()

    try:
        main()
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.exception(f"ç¨‹åºæ‰§è¡Œä¸­å‘ç”Ÿæœªå¤„ç†çš„é”™è¯¯: {e}")
    finally:
        logger.info("ç¨‹åºç»“æŸ")